{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from openicl import DatasetReader\n",
    "\n",
    "\n",
    "# Loading dataset from huggingface\n",
    "dataset = load_dataset('/data/lhb/huggingface/dataset/sst2_gpt3mix')\n",
    "\n",
    "# Define a DatasetReader, with specified column names where input and output are stored.\n",
    "data = DatasetReader(dataset, input_columns=['text'], output_column='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openicl import PromptTemplate\n",
    "tp_dict = {\n",
    "    0: \"</E>Positive Movie Review: </text>\",\n",
    "    1: \"</E>Negative Movie Review: </text>\" \n",
    "}\n",
    "\n",
    "template = PromptTemplate(tp_dict, {'text': '</text>'}, ice_token='</E>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-03-09 20:15:28,376] [openicl.icl_retriever.icl_topk_retriever] [INFO] Creating index for index set...\n",
      "  0%|          | 0/6920 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|██████████| 6920/6920 [01:47<00:00, 64.54it/s]\n"
     ]
    }
   ],
   "source": [
    "from openicl import TopkRetriever\n",
    "from openicl.icl_retriever.icl_cone_retriever import ConERetriever\n",
    "# 这一步会将 训练集中的 所有数据 进行 embedding 在内存中 构建一个 faiss 索引，只有在 检索的时候 才会对所有 测试集中的 数据 进行 embedding 和 建立索引\n",
    "\n",
    "retriever = TopkRetriever(data, ice_num=8)\n",
    "# retriever = ConERetriever(data, candidate_num = 30, ice_num=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openicl import PPLInferencer\n",
    "inferencer = PPLInferencer(model_name='/data/lhb/huggingface/model/tokenizer/gpt2-xl', output_json_filepath = \"./icl_inference_output\", output_json_filename = \"predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'template'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mopenicl\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AccEvaluator\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# the inferencer requires retriever to collect in-context examples, as well as a template to wrap up these examples.\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43minferencer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretriever\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mice_template\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemplate\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/lhb/test-openicl-0.1.8/OpenICL/openicl/icl_inferencer/icl_ppl_inferencer.py:71\u001b[0m, in \u001b[0;36mPPLInferencer.inference\u001b[0;34m(self, retriever, ice_template, prompt_template, output_json_filepath, output_json_filename, normalizing_str)\u001b[0m\n\u001b[1;32m     68\u001b[0m     output_json_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_json_filename\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# 2. Get results of retrieval process\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m ice_idx_list \u001b[38;5;241m=\u001b[39m \u001b[43mretriever\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# 3. Get labels of all the classes\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/data/lhb/test-openicl-0.1.8/OpenICL/openicl/icl_retriever/icl_cone_retriever.py:177\u001b[0m, in \u001b[0;36mConERetriever.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mretrieve\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 177\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtopk_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/lhb/test-openicl-0.1.8/OpenICL/openicl/icl_retriever/icl_cone_retriever.py:110\u001b[0m, in \u001b[0;36mConERetriever.topk_search\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    107\u001b[0m rtr_idx_list \u001b[38;5;241m=\u001b[39m [[] \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(res_list))]\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m# key word is the word in the template to predict the label\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m key_word \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mice_template\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtemplate\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m</text>\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msplit()[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    112\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrieving data for test set...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(res_list, disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_main_process):\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'template'"
     ]
    }
   ],
   "source": [
    "from openicl import AccEvaluator\n",
    "# the inferencer requires retriever to collect in-context examples, as well as a template to wrap up these examples.\n",
    "predictions = inferencer.inference(retriever, ice_template=template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.8396485447556288}\n"
     ]
    }
   ],
   "source": [
    "# compute accuracy for the prediction\n",
    "score = AccEvaluator().score(predictions=predictions, references=data.references)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1821"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1821"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "def score(predictions, references):\n",
    "    assert len(predictions) == len(references)\n",
    "    mapping_to_int_dict = {label: idx for idx, label in enumerate(set(map(str, references)))}\n",
    "    pred_set = set(predictions)\n",
    "    for pred in pred_set:\n",
    "        if str(pred) not in mapping_to_int_dict.keys():\n",
    "            mapping_to_int_dict[str(pred)] = len(mapping_to_int_dict)\n",
    "    golds = [mapping_to_int_dict[str(gold)] for gold in references]\n",
    "    preds = [mapping_to_int_dict[str(pred)] for pred in predictions]\n",
    "    metric = evaluate.load(\"/data/lhb/huggingface/metrics/accuracy\")\n",
    "    return metric.compute(references=golds, predictions=preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8396485447556288}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score(references=data.references, predictions=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<openicl.icl_retriever.icl_cone_retriever.ConERetriever at 0x7f52283ea3e0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将 topk_search 函数拆分调试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# 测试集前向传播，得到 {'embed'...，'metadata':..}\n",
    "np.random.seed(42)\n",
    "res_list = retriever.forward(retriever.dataloader)\n",
    "rtr_idx_list = [[] for _ in range(len(res_list))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'template'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m key_word \u001b[38;5;241m=\u001b[39m \u001b[43mretriever\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mice_template\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtemplate\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m</text>\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msplit()[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'template'"
     ]
    }
   ],
   "source": [
    "key_word = retriever.ice_template.template[0].split('</text>')[-1].split()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mretriever\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mice_template\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtemplate\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m</text>\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "retriever.ice_template.template[0].split('</text>')[-1].split()[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openicl0.1.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
